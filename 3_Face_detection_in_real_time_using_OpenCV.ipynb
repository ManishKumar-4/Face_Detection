{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Face detection in real time using OpenCV\n",
    "\n",
    "The most common way to detect a face (or any objects), is using the \"Haar Cascade classifier\"\n",
    "\n",
    "Object Detection using Haar feature-based cascade classifiers is an effective object detection method proposed by Paul Viola and Michael Jones in their paper, \"Rapid Object Detection using a Boosted Cascade of Simple Features\" in 2001. It is a machine learning based approach where a cascade function is trained from a lot of positive and negative images. It is then used to detect objects in other images.\n",
    "\n",
    "Here we will work with face detection.Initially, the algorithm needs a lot of positive images (images of faces) and negative images (images without faces) to train the classifier. Then we need to extract features from it. The benefit of OpenCV is that it comes with a trainer as well as a detector. If you want to train our own classifier for any object like car, planes etc. we can use OpenCV to create one.\n",
    "\n",
    "If we do not want to create our own classifier, OpenCV already contains many pre-trained classifiers for face, eyes, smile, etc. Those XML files can be download from haarcascades directory.\n",
    "\n",
    "Basic steps are:\n",
    "1)Install open cv\n",
    "2)Import the open cv library\n",
    "3)Loading a pretrained frontal face classifier\n",
    "4)Capture video from webcam using VideoCapture() function\n",
    "5)Read the frame from the webcam\n",
    "6)As openCV require image in gray so convert the image to gray\n",
    "5)Detect the face using pretrained classifier and using detectMultiscale function on the gray image\n",
    "6)Extract the coordinates and use the rectangle function to draw the rectangle around the face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\manish\\anaconda\\lib\\site-packages (4.4.0.46)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\manish\\anaconda\\lib\\site-packages (from opencv-python) (1.18.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#install openCv\n",
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2.0\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loadinng the pretrained classifier-use proper file and its file path\n",
    "classifier=cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With OpenCV, we can capture a video from the camera. It lets us create a video capture object which is helpful to capture videos through webcam and then we may perform desired operations on that video\n",
    "\n",
    "Steps to capture a video:\n",
    "\n",
    "Use cv2.VideoCapture() to get a video capture object for the camera.\n",
    "\n",
    "Set up an infinite while loop and use the read() method to read the frames using the above created object.\n",
    "\n",
    "Use cv2.imshow() method to show the frames in the video.\n",
    "\n",
    "Breaks the loop when the user clicks a specific key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a video capture object #Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). We can select the second camera by passing 1 and so on.\n",
    "vid_cap=cv2.VideoCapture(0)\n",
    "# To use a video file as input \n",
    "# cap = cv2.VideoCapture('filename.mp4')\n",
    "    \n",
    "while True:\n",
    "    #capture frame by frame\n",
    "    ret,frame=vid_cap.read()\n",
    "    # Basically, ret is a boolean regarding whether or not there was a return at all, at the frame is each frame that is returned. If there is no frame, you wont get an error, you will get None.\n",
    "    #frame = cv2.flip(frame, -1) # Flip camera vertically\n",
    "    \n",
    "    #converting the frame/image to grayscale as per requirement of openCv\n",
    "    gray_image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #apply pretrained haar classifier to detect faces,we basically get coordinates of face\n",
    "    #scaleFactor is the parameter specifying how much the image size is reduced at each image scale. It is used to create the scale pyramid.\n",
    "    #minNeighbors is a parameter specifying how many neighbors each candidate rectangle should have, to retain it. A higher number gives lower false positives.\n",
    "    #minSize is the minimum rectangle size to be considered a face.#minSize=(20, 20)\n",
    "    faces=classifier.detectMultiScale(gray_image,scaleFactor=1.2,minNeighbors=5)\n",
    "    #note we use detectMultiScale function on gray image\n",
    "   \n",
    "    \n",
    "    \n",
    "    #Draw rectangle around each face on the frame not on the grayimage\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        #roi_gray = gray_image[y:y+h, x:x+w]\n",
    "        #roi_color = frame[y:y+h, x:x+w]  \n",
    "    \n",
    "    #Displaying the image using imshow method\n",
    "    cv2.imshow('Detected Faces',frame)\n",
    "    \n",
    "    #desired button of choice for quitting\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "    \n",
    "# Release the VideoCapture object.\n",
    "vid_cap.release()\n",
    "\n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2)Face and eye detection in real time using openCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For eye detection we need to first detect the face and then detect the eye.So first we will find the face coordinates and then in that we will use pretrained eye classifier to get the location of eyes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loadinng the pretrained classifier-use proper file and its file path\n",
    "classifier=cv2.CascadeClassifier('haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "#Loading the pretrained eye classifier\n",
    "eye_classifier=cv2.CascadeClassifier('haarcascades/haarcascade_eye.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a video capture object #Its argument can be either the device index or the name of a video file. Device index is just the number to specify which camera. Normally one camera will be connected (as in my case). So I simply pass 0 (or -1). We can select the second camera by passing 1 and so on.\n",
    "vid_cap=cv2.VideoCapture(0)\n",
    "# To use a video file as input \n",
    "# cap = cv2.VideoCapture('filename.mp4')\n",
    "    \n",
    "while True:\n",
    "    #capture frame by frame\n",
    "    ret,frame=vid_cap.read()\n",
    "    \n",
    "    #converting the frame/image to grayscale as per requirement of openCv\n",
    "    gray_image=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #Applying pretrained classifier with detectMultiScale function to detect faces\n",
    "    faces=classifier.detectMultiScale(gray_image,scaleFactor=1.1,minNeighbors=5,minSize=(20,20))\n",
    "   \n",
    "    \n",
    "    #Draw rectangle around each face on the frame not on the grayimage\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),2)\n",
    "        roi_gray = gray_image[y:y+h, x:x+w]  #taking out only face from the whole gray image\n",
    "        roi_color = frame[y:y+h, x:x+w]      #taking out only face portion from the whole color image\n",
    "        \n",
    "        #Now once we get the face its time to detect eyes,so we will use pretrained eye classifier with detectMultiScale function on gray face part of the image\n",
    "        eyes = eye_classifier.detectMultiScale(roi_gray)\n",
    "        \n",
    "        for (ex,ey,ew,eh) in eyes:\n",
    "            #drawing the rectangle shape around eye in colored image\n",
    "            cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,0,255),2)\n",
    "            \n",
    "    \n",
    "    #Displaying the image using imshow method\n",
    "    cv2.imshow('Face and its corresponding eye detection',frame)\n",
    "    \n",
    "    #desired button of choice for quitting\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): \n",
    "        break\n",
    "    \n",
    "# Release the VideoCapture object.\n",
    "vid_cap.release()\n",
    "\n",
    "# Destroy all the windows \n",
    "cv2.destroyAllWindows() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
